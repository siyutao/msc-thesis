\chapter{Experiments and analysis}
\section{Experiment 1: Transitivity ratios}\label{sec:exp1}
\subsection{From transitivity categories to transitivity ratios}
If we were to approach the task of verb classification, i.e., to categorize verbs of a language into verb classes according to their syntacto-semantic properties and behavior, we could well imagine ourselves with fine-grained verb classes ร la \citet{levin1993} in the end, but will likely have to start with more basic distinctions and, first among them, that of verb \textit{transitivity}. 

In contrast with more fine-grained metrics of valency to be used in further experiments, the notion of \textit{transitivity}, i.e., whether a verb can take one or more objects, is probably familiar to any of us who has ever been in a foreign language classroom. Traditionally, a binary distinction is made between \textit{intransitive} verbs, which take only a subject and no objects and \textit{transitive} verbs, which take one or more objects. Further categories, some overlapping, deal with finer distinctions, such as \textit{ditransitive} verbs (those taking two objects), ambitransitive verbs (those that can be used both transitively and intransitively), etc.

While transitivity categories can and are already used in verb classification, as well as the cross-lingual comparison thereof, a functional and qualitative approach demands a better picture of transitivity \textit{use} in language. An example illustrating what transitivity categories do \textit{not} capture comes from the study of near-synonyms: \citep{biber1998} compares the English verbs \textit{begin} and \textit{start} in a corpus-based study. At first glance, English appears to have provided us with two verbs that are not only semantically synonymous but share valency properties as well, as they can both be used in transitive and intransitive constructions:

\begin{exe}
\ex\label{example-begin_start}
  \begin{xlist}
  \ex{I had better issue a survival kit before we \textit{start}/\textit{begin}.\\ \strut\hfill \textbf{intransitive}}
  \ex{Then they \textit{started}/\textit{began} the quota system.\\ \strut\hfill \textbf{transitive with noun phrase}}
  \ex{They'd \textit{started}/\textit{begun} leaving before I arrived.\\ \strut\hfill \textbf{transitive with \textit{-ing} clause}}
  \ex{One of the wheels had \textit{started}/\textit{begun} to wobble.\\ \strut\hfill \textbf{transitive with \textit{to} clause}}
  \end{xlist}
\end{exe}

This however belies the different usage patterns exhibited by these verbs, as \citet[95]{biber1998} demonstrate with statistics from the British National Corpus (BNC): while both uses are clearly grammatical for both verbs, \textit{begin} is used more often in a transitive frame than \textit{start} across different genres in the BNC: in fiction, 78\% of \textit{begin} occurrences (196/250) are with various transitive patterns vs. only 60\% for \textit{start} (149/250); transitive uses are less frequent in academic texts in general but the observation of relatively higher transitivity for \textit{begin} still holds (57\% vs. 36\%, or 110/192 vs. 51/142).

To capture such differences in degrees of transitivity, we propose measuring the \textbf{transitivity ratio} based on language corpora, defined as the percentage of verb use that are transitive. Necessarily then, and in contrast with transitivity categories, any values are calculated on an \textit{ad hoc} basis in a given corpus. Unless we expect the corpus to be a representative sample of all language use in that language, which is certainly not the case for the UD corpora we are using, the absolute values of these ratios at a lexeme-level cannot be directly interpreted. Instead, intralinguistic analysis will take the form of analyzing the distribution of verbs according to their observed transitivity ratios. We take on an additional assumption, that the corpus would reflect the general tendency towards (in-)transitivity of the language, if too piecemeal for individual verbs, which then allows for cross-lingual comparison of transitivity ratios at a token-level.

The UD annotation scheme provides us with good facility to investigate transitivity at lexeme- and token-levels, as the relevant dependency relations \textsc{nsubj} and \textsc{obj} mark respectively the first and second core arguments of a verb with their typical syntactic roles as subject and object. This is defined without respect to specific cases (even though typically the accusative in languages with a case system) or semantic roles (even though they would typically be the proto-agent and proto-patient) in an effort to avoid \textit{a priori} categories to the extent possible. The renaming of the \textsc{dobj} relation to \textsc{obj}, among the changes introduced by UD v2 \citep{nivre2020}, reflects the same effort.

Given the clear and typologically sound UD dependency relation annotations, one can be forgiven for already jumping at the task. On closer examination, however, we see that arriving at a clear definition of transitivity ratio is not trivial. We consider four different definitions of a quantitative transitivity ratio within the UD annotation scheme here:

\begin{enumerate}
    \item the ratio of verb instances with both \textsc{nsubj} and \textsc{obj} dependents, as compared to verb instances with an \textsc{nsubj} dependent
    \item the ratio of verb instances with an \textsc{obj} dependent, as compared to verb instances with an \textsc{nsubj} dependent
    \item the ratio of verb instances with an \textsc{obj} dependent, as compared to all verb instances
    \item the ratio of verb instances with an \textsc{obj} dependent, as compared to verb instances with either an \textsc{nsubj} or an \textsc{obj} dependent
\end{enumerate}

Def. 1 is an attempt at enforcing a definition of the transitive object as the \textit{second} core argument of the verb by excluding from calculation instances where the first core argument (i.e., subject) is not realized. This turns out counterproductive for two reasons. Firstly, this does not sit well with the core definition on transitivity, as instances of verb use where the subject is not expressed should not count against the fact that the verb is taking a transitive object; secondly, this is undesirable in practice when accounting for typological variations, as the metric would be biased against pro-drop languages that drops subject pronouns more often than objects such as Spanish.

Revising def. 1 and dropping the requirement in the numerator for verbs to have an \textsc{nsubj} dependent gives us def. 2. However, this is not sufficient, as we are now concerned with the opposite problem where subject-dropping languages are likely to have a smaller denominator, resulting in a high transitivity ratio that is not representative. Def. 3 is where we drop the \textsc{nsubj} requirement from the denominator as well. This still faces problems, as verb instances where both subject and object are dropped would affect the denominator, and such usage, e.g., non-predicative usage of verbs, is unlikely to be equally frequent in different languages and would therefore interfere with the cross-lingual comparability of our transitivity ratio focusing on argument structure of verb predicates. Taking all these potential drawbacks into consideration, we arrive at def. 4 with the number of verb instances with either an \textsc{nsubj} or an \textsc{obj} dependent in the denominator. 

\input{tables/transitivity_defs.tex}

While there is a strong case for Def. 4 being the most principled definition, we nevertheless implement all four definitions in this experiment to empirically verify the intuitions. They are also represented with feature matrices in Tab.~\ref{tab:transitivity-defs} for quick reference.

\subsection{Transitivity ratios and the lexicon}

We go through all eligible UD corpora and first compile transitivity ratio statistics for each verb lexeme based on the definitions. From there, we compute the per-language statistics that will become the basis for our cross-lingual comparison: we calculate the lexeme-level and token-level transitivity ratios for each language, respectively the arithmetic mean of the lexeme transitivity ratios and the mean of lexeme transitivity ratios weighted by the frequency of the lexeme. In addition to our transitivity ratio metrics, we will also calculate an additional metric, percentage of transitive verbs, i.e., the percentage of verbs in the observed lexicon that are not strictly intransitive (defined as never observed to take an \textsc{obj}), for comparison purposes, as it should correspond better with the traditional binary distinction between transitive and intransitive verbs.

We perform the experiment on the selected subset of UD data as described in ยง\ref{subsec:data_ud}. For the analysis, we include only languages with at least 100 observed verb lexemes (56 out of 79 languages); the full results from the experiments can be found in the accompanying data and appendices. 

\input{tables/transitivity_spearmanr.tex}

To compare between the different definitions of transitivity, we compute Spearman's rank correlations between the lexeme- and token-level means of transitivity ratios according to each of our four definitions, as well as between the transitive verb percentage and each of them. The correlation statistics are listed in Tab.~\ref{tab:transitivity_spearmanr}. We observe overall strong correlations between the mean transitivity ratios at lexeme- and token-levels for all four definitions, with the highest observed for definition 4 ($\rho(54)=.87, p=.000$) and lowest observed for definition 3 ($\rho(54)=.77, p=.000$). The strong correlation is not surprising as we have no reason to expect the more frequent verbs to behave differently from the less frequent verbs with regard to transitivity ratios. This can also be confirmed by correlation tests between verb frequency and verb transitivity ratios for each language, which show no strong correlation.

The correlation statistics between the transitive verb percentages and the transitivity ratios are slightly more revealing. If nothing else, they help us eliminate definition 2 from the competition as it shows no statistically significant correlation ($\rho(54)=-.17, p=.221$ for lexeme-level transitivity ratios and $\rho(54)=-.03, p=.817$ for token-level) while strong correlations are observed for all three other definitions (see Tab.~\ref{tab:transitivity_spearmanr}).

In the absence of a practical reason based on correlation statistics to select a definition among the remaining three over the others, we proceed with the rest of the analysis with results using def. 4, which we have considered \textit{a priori} to be the most principled. We proceed with the intralinguistic analyses by plotting histograms of the distributions of verbs among the transitivity ratios in different languages, as shown in Fig.~\ref{fig:verb_dist_transitivity}.

\begin{sidewaysfigure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/verb_dist_by_transitivity.pdf}
  \caption{Histograms showing the binned distributions of verbs according to their transitivity ratio in different languages}
  \label{fig:verb_dist_transitivity}
\end{sidewaysfigure}

Most distributions are bimodal with peak at both ends, which supports the overall cross-lingual validity of a binary conception of transitivity. But exceptions abound as well, among others Indonesian (unimodal with peak in the middle), Catalan, Galician, Spanish (unimodal with peak on right). And even between the bimodal distributions, we note that they are rarely symmetric, with differing levels of skew towards either end. 

\subsection{Genetic and areal patterns in transitivity}

\input{tables/most_least_transitive_lang.tex}

Tab.~\ref{tab:most_tr_by_verb_percentage} and \ref{tab:most_tr_by_token_mean} list the most and least `transitive' languages in our study, respectively according to the transitive verb percentage and the token-level transitivity ratio. Recall that we favor token-level transitivity ratios over lexeme-level ones for cross-lingual comparison.

Among the most transitive languages are Romance (Catalan, Spanish, French, Galician) and Germanic (German, Norwegian, English, Danish) languages of Europe, Sinitic languages (Chinese, Classical Chinese, particularly when measured by token-level transitivity ratio), Indonesian, Hindi. On the opposite end of the spectrum are Hebrew, Irish, Japanese, as well as Baltic (Lithuanian, Latvian) and Slavic (Slovak, Russian, Polish) languages, which have the lowest transitivity ratios.

\begin{sidewaysfigure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/transitivity_europe.pdf}
  \caption{Mean transitivity ratios in languages of Europe}
  \label{fig:transitivity_europe}
\end{sidewaysfigure}

To look at any potential areal patterns in transitivity, we also map our token-level transitivity ratio results for European languages in Fig.~\ref{fig:transitivity_europe}. (A less Eurocentric study of areal patterns is unfortunately difficult for the lack of enough language samples in the UD.) We find a particularly high transitivity area in the Iberian peninsular as well as another relatively high transitivity area in the Balkans, in contrast to eastern and northern Europe with lower transitivity.

Where the languages overlap, these observations match well with those from \citet{say2014}'s survey of transitivity in European languages (as measured by the percentage of verbs that are transitive from a predetermined list), who observed high transitivity areas in western Europe except Irish and south-western Balkans, and a corresponding low transitivity area in eastern Europe.

% Dutch appears to be a special case in several ways. \todo{case study: Dutch - divergence btwn two metrics, maybe due to ergative verbs? and diff btwn Dutch vs Afrikanns - significant grammatical differences?}

% diff between tr. verb -> token-level tr. ratio reflects also the fact that tr. verbs can often be used intransitively
% how "free" a language is to use tr. verb  intr./vice versa is another interesting further study 


\section{Experiment 2: Valency frame entropy}
\subsection{Valency frame encoding in UD}

Psycholinguistic studies consistently show the effect of semantic and syntactic attributes of the verb on online sentence processing \citep{shapiro1987,collina2001}. More specifically, studies suggest that such attributes are organized in frames as opposed to parameters such as transitivity: e.g., \citet{shetreet2007}, an fMRI study in Hebrew, shows that the number of options in terms of subcategorization and thematic frames is better correlated to activity in the cortical areas that are associated with linguistic processing, as opposed to the number of complements.

Besides the information about transitivity, UD annotations make available further information that are related to the morphosyntactic encoding of valency frame, including information about further dependents, word order, as well as case and adpositional markings. In this experiment, we make use of such information for a more fine-grained characterization of the variation in valency frame encoding for different verbs using entropy-based measures.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    \textbf{UD label} & \textbf{Dependent} \\
    \midrule
    \textsc{nsubj} & nominal subject \\
    \textsc{obj} & object \\
    \midrule
    \textsc{obl} & oblique nominal \\
    \textsc{iobj} & indirect object \\
    \textsc{advmod} & adverbial modifier \\
    \textsc{ccomp} & clausal complement \\
    \textsc{xcomp} & open clausal complement \\
    \textsc{advcl} & adverbial clause modifier \\
    \bottomrule
  \end{tabular}
  \caption{UD dependent labels included in valency frame extraction}\label{tab:ud-dependent-labels}
\end{table}

To compute the entropy measures, we need to first extract the \textbf{valency frame encoding} information from the UD annotations at a token level for each verb instance. We start by determining the scope of our investigation and select a list of dependency relations in Tab.~\ref{tab:ud-dependent-labels} which categorizes different syntactic dependencies that can be attached to the verb. These include core dependents, namely \textsc{nsubj} and \textsc{obj}, but also so-called non-core dependents as well. \todo{justification for that: syntactic classification of verb using non-core information, cite previous work} It is important to note here that it is the cross-lingual taxonomy of the dependencies that serves as the cross-linguistic basis of comparison and that the name of the labels, while linguistically informed, do not affect the entropy calculation in any way.

We extract information about (1) the presence or absence of dependency relations attached to a verb token, (2) the word order information, whether specific dependents precede or follow the head verb, and (3) morphological case marking on the dependents, if any.

\todo{example for what is to be extracted}

\subsection{Valency frame entropy}

We propose \textbf{valency frame entropy}, conditioned on the verb, as a measure of the variation of valency frame encoding for a verb. This is formalized as the joint entropy of the relevant UD dependency relations, i.e., number of bits needed to encode the entire valency frame.

Let $X_1, X_2,\ldots,X_n$ be discrete random variables where each $X_k$ represents a UD dependency relation (e.g., \textsc{nsubj}, \textsc{obj}, \ldots). These variables have corresponding sample spaces (images? support?) $\mathcal{X}_1, \mathcal{X}_2, \ldots, \mathcal{X}_n$, which represent the possible levels or outcomes of each variable with regard to its presence, linearized order, and any case information.

Additionally, let there be a variable $V$ that represents the choice of a verb from the lexicon $\mathcal{V}$. We are first interested in calculating the entropy of each dependency relation given a specific verb $v \in \mathcal{V}$. This is defined as:

$$
H(X_{k}|V=v)=
-\sum\limits_{x_{k}\in{}\mathcal{X}_{k}}{P(x_{k}|V=v)\log_{2}{P(x_{k}|V=v)}}
$$

Here, $P(x_k | V = v)$ represents the conditional probability of the outcome $x_k$ of the dependency relation $X_k$ given that the verb variable $V$ takes on the value $v$. The entropy of $X_k$ is calculated by summing the products of these probabilities with their logarithms (base 2) taken, each corresponding to a possible outcome $x_k$.

The joint entropy of all dependent relations, denoted as $H_{\text{joint}}(X_1, X_2, \ldots, X_n | V = v)$, quantifies the uncertainty associated with the combined set of random variables $X_1, X_2, \ldots, X_n$, again given a specific value $v$ for the verb variable $V$. This is defined as:

$$
H_{\text{joint}}(X_1, X_2, \ldots, X_n | V=v) =
-\sum\limits_{x_1\in{}\mathcal{X}_1}\cdots\sum\limits_{x_n\in{}\mathcal{X}_n}{P(x_1, x_2, \ldots,x_{n}|V=v)\log_2P(x_1, x_2, \ldots,x_n|V=v)}
$$

Here, $P(x_1, x_2, \ldots, x_n | V = v)$ represents the joint probability distribution of the outcomes $x_1, x_2, \ldots, x_n$ of the random variables $X_1, X_2, \ldots, X_n$, given that the verb variable $V$ takes on the value $v$. The joint entropy is calculated by summing the products of these joint probabilities with their logarithms (base 2) taken, each corresponding to a specific combination of outcomes $x_1, x_2, \ldots, x_n$.

% To reduce the effect of artifacts, we split each distribution randomly at a fixed ratio (e.g. $1:1$) into two, resulting in two distributions $X_1,\ldots,X_n$ and $X^{\prime}_{1},\ldots,X^{\prime}_{n}$. 

% For each of the argument slot, the cross-entropy between the distributions $X_k$ and $X_k^{\prime}$ with the same image $\mathcal{X}_{k}$ is

% $$
% H_{cross}{(X_k,X_k^{\prime}|V=v)}=
% -\sum\limits_{x\in{}\mathcal{X_{k}}}{P(x|V=v)\log_{2}{P^{\prime}(x|V=v)}}
% $$

% And the joint cross-entropy of  $X_1,\ldots,X_n$ and $X_1^{\prime},\ldots,X_n^{\prime}$ would be

% $$
% H_{joint-cross}(X_{1},\ldots,X_{n},X_{1}^{\prime},\ldots,X_{n}^{\prime}|V=v)=
% -\sum\limits_{x_1\in{}\mathcal{X}_1}\cdots\sum\limits_{x_n\in{}\mathcal{X}_n}{P(x_1,\ldots,x_{n}|V=v)\log_{2}P^{\prime}(x_1,\ldots,x_n|V=v)}
% $$


\subsection{Core and non-core dependents}



\begin{sidewaysfigure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/joint_entropy_freq.pdf}
  \caption{Scatter plots showing relationship between valency frame entropy and frequency rank of verbs, color shows number of frames associated with the verb}
  \label{fig:joint_entropy_freq}
\end{sidewaysfigure}

\section{Experiment 3: Word order and case information}
\subsection{Motivation}
\subsection{Experiment design: ablation studies}
\subsection{Results}

\section{Experiment 4: Verb entropy}
\subsection{Motivation}
\subsection{Experiment design}
\subsection{Results}

\section{Experiment 5: Verb-finalness}
\subsection{Motivation}
\subsection{Experiment design}
\subsection{Results}