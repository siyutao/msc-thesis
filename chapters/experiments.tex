\chapter{Experiments and analysis}
\section{Experiment 1: Transitivity ratios}\label{sec:exp1}
\subsection{From transitivity categories to transitivity ratios}
Regardless of how we approach the task of verb classification, i.e., to categorize the verbs of a language into verb classes according to their syntacto-semantic properties and behavior, we would expect to arrive at fine-grained verb classes ร la \citet{levin1993} in the end. But such an expectation does not render obsolete the cruder distinctions like that of verb \textit{transitivity}. In part, this is due to their utility as convenient starting points of comparison for verbs within a language, but their simplicity also translates to being more cross-lingually valid bases of typological comparison. 

This first experiment deals with metrics of \textit{transitivity}, i.e., the ability of a verb to take one or more objects. It is surely a more familiar and intuitive concept as compared to the finer-grained metrics of valency to follow in the experiments. In traditional grammars, a basic binary distinction is made between \textit{intransitive} verbs, which take only a subject and no objects and \textit{transitive} verbs, which take one or more objects. Additional categories, some overlapping, make finer distinctions, such as \textit{ditransitive} verbs (those taking two objects), \textit{ambitransitive} verbs (those that can be used both transitively and intransitively), etc.

I focus here on the simple distinction between transitive and intransitive verbs, but, in keeping with a functional and quantitative outlook, binary categories do not sufficiently capture the nuances of verb use. An example illustrating why comes from the study of near-synonyms: \citep{biber1998}, an early corpus linguistics study, compares the English verbs \textit{begin} and \textit{start} in the British National Corpus (BNC). At first glance, English appears to have provided us with two verbs that are not only semantically synonymous but share valency properties as well, as they can both be used in transitive and intransitive constructions:

\begin{exe}
\ex\label{example-begin_start}
  \begin{xlist}
  \ex{I had better issue a survival kit before we \textit{start}/\textit{begin}.\\ \strut\hfill \textbf{intransitive}}
  \ex{Then they \textit{started}/\textit{began} the quota system.\\ \strut\hfill \textbf{transitive with noun phrase}}
  \ex{They'd \textit{started}/\textit{begun} leaving before I arrived.\\ \strut\hfill \textbf{transitive with \textit{-ing} clause}}
  \ex{One of the wheels had \textit{started}/\textit{begun} to wobble.\\ \strut\hfill \textbf{transitive with \textit{to} clause}}
  \end{xlist}
\end{exe}

This however belies their different usage patterns as observed in the BNC: while both uses are clearly grammatical for both verbs, \textit{begin} is used more often in a transitive frame than \textit{start} across different genres in the BNC: in fiction, 78\% of \textit{begin} occurrences (196/250) are with various transitive patterns vs. only 60\% for \textit{start} (149/250); transitive uses are less frequent in academic texts in general but the observation of relatively higher transitivity for \textit{begin} still holds (57\% vs. 36\%, or 110/192 vs. 51/142).

To capture such differences in levels of transitivity, I propose measuring the \textbf{transitivity ratio} based on language corpora, defined as the percentage of verb instances that are transitive. The verbs of a language can then be placed on a spectrum of transitivity, with strictly intransitive verbs on the one end, strictly transitive verbs on the opposite end, and all other verbs\footnote{Note that not all of them are ambitransitive verbs, as my approach is only concerned with the surface realization of transitivity. In particular, this means instances of pro-drop of objects are counted as intransitive, where some would argument for a null object analysis instead.} somewhere in between.

Necessarily then, and in contrast with transitivity categories, any values are calculated on an \textit{ad hoc} basis in a given corpus. Unless we expect the corpus to be a representative sample of all language use in that language, which is certainly not the case for the UD corpora used here, the absolute values of these ratios at a lexeme-level cannot be directly interpreted. Instead, intralinguistic analysis will take the form of analyzing the distribution of verbs according to their observed transitivity ratios. I take on an additional assumption, that the corpus would reflect the general tendency towards (in-)transitivity of the language, if too piecemeal for individual verbs, which justifies the cross-lingual comparison of transitivity ratios at a token-level as well.

The UD annotation scheme facilitates the investigation of transitivity at lexeme- and token-levels, as the relevant dependency relations \textsc{nsubj} and \textsc{obj} mark respectively the first and second core arguments of a verb with their typical syntactic roles as subject and object. This is defined without respect to specific cases (even though typically the accusative in languages with a case system) or semantic roles (even though they would typically be the proto-agent and proto-patient) in an effort to avoid \textit{a priori} categories to the extent possible. The renaming of the \textsc{dobj} relation to \textsc{obj}, among the changes introduced by UD v2 \citep{nivre2020}, reflects the same laudable effort.

Despite the typologically sound UD dependency relation annotations, arriving at a clear definition of transitivity ratio in the UD context is still not trivial upon close examination. In fact, let us consider four different definitions of a quantitative transitivity ratio within the UD annotation scheme here:

\begin{enumerate}
    \item the ratio of verb instances with both \textsc{nsubj} and \textsc{obj} dependents, as compared to verb instances with an \textsc{nsubj} dependent
    \item the ratio of verb instances with an \textsc{obj} dependent, as compared to verb instances with an \textsc{nsubj} dependent
    \item the ratio of verb instances with an \textsc{obj} dependent, as compared to all verb instances
    \item the ratio of verb instances with an \textsc{obj} dependent, as compared to verb instances with either an \textsc{nsubj} or an \textsc{obj} dependent
\end{enumerate}

Def. 1 is an attempt at enforcing a definition of the transitive object as the \textit{second} core argument of the verb by excluding from calculation instances where the first core argument (i.e., subject) is not realized. This turns out counterproductive for two reasons. Firstly, this does not sit well with the core definition on transitivity, as instances of verb use where the subject is not expressed should not count against the fact that the verb is taking a transitive object; secondly, this is undesirable in practice when accounting for typological variations, as the metric would be biased against pro-drop languages that drops subject pronouns more often than objects such as Spanish.

Revising def. 1 and dropping the requirement in the numerator for verbs to have an \textsc{nsubj} dependent gives us def. 2. However, this is not sufficient, as the opposite problem surfaces where subject-dropping languages are likely to have a smaller denominator, resulting in a high transitivity ratio that is not representative. Def. 3 drops the \textsc{nsubj} requirement from the denominator as well. This still faces problems, as verb instances where both subject and object are dropped would affect the denominator, and such usage, e.g., non-predicative usage of verbs, is unlikely to be equally frequent in different languages and would therefore interfere with the cross-lingual comparability of our transitivity ratio focusing on argument structure of verb predicates. Taking all these potential drawbacks into consideration, we arrive at def. 4 with the number of verb instances with either an \textsc{nsubj} or an \textsc{obj} dependent in the denominator. 

\input{tables/transitivity_defs.tex}

While there is a strong case for Def. 4 being the most principled definition, I nevertheless implement all four definitions in this experiment to empirically verify the intuitions. They are also represented with feature matrices in Tab.~\ref{tab:transitivity-defs} for quick reference.

\subsection{Transitivity ratios and the lexicon}

Transitivity ratio statistics for each verb lexeme based on the definitions are first compiled. From there, per-language statistics that will become the basis for our cross-lingual comparison are computed: I calculate the lexeme-level and token-level transitivity ratios for each language, respectively the arithmetic mean of the lexeme transitivity ratios and the mean of lexeme transitivity ratios weighted by the frequency of the lexeme. In addition to the transitivity ratio metrics, an additional metric, percentage of transitive verbs, i.e., the percentage of verbs in the observed lexicon that are not strictly intransitive (defined as never observed to take an \textsc{obj}), is calculated for comparison purposes, as it should correspond better with the traditional binary distinction between transitive and intransitive verbs.

I perform the experiment on the selected subset of UD data as described in ยง\ref{subsec:data_ud}. For the analysis, I include only languages with at least 100 observed verb lexemes (56 out of 79 languages); the full results from the experiments can be found in the accompanying data and appendices. 

\input{tables/transitivity_spearmanr.tex}

To compare between the different definitions of transitivity, we compute Spearman's rank correlations between the lexeme- and token-level means of transitivity ratios according to each of our four definitions, as well as between the transitive verb percentage and each of them. The correlation statistics are listed in Tab.~\ref{tab:transitivity_spearmanr}. We observe overall strong correlations between the mean transitivity ratios at lexeme- and token-levels for all four definitions, with the highest observed for definition 4 ($\rho(54)=.87, p=.000$) and lowest observed for definition 3 ($\rho(54)=.77, p=.000$). The strong correlation is not surprising as we have no reason to expect the more frequent verbs to behave differently from the less frequent verbs with regard to transitivity ratios. This can also be confirmed by correlation tests between verb frequency and verb transitivity ratios for each language, which show no strong correlation.

The correlation statistics between the transitive verb percentages and the transitivity ratios are slightly more revealing. If nothing else, they help us eliminate definition 2 from the competition as it shows no statistically significant correlation ($\rho(54)=-.17, p=.221$ for lexeme-level transitivity ratios and $\rho(54)=-.03, p=.817$ for token-level) while strong correlations are observed for all three other definitions (see Tab.~\ref{tab:transitivity_spearmanr}).

In the absence of a practical reason based on correlation statistics to select a definition among the remaining three over the others, the rest of the analysis uses def. 4, which I have considered \textit{a priori} to be the most principled. Further intralinguistic analyses are done by plotting histograms of the distributions of verbs among the transitivity ratios in different languages, as shown in Fig.~\ref{fig:verb_dist_transitivity}.

\begin{sidewaysfigure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/verb_dist_by_transitivity.pdf}
  \caption{Histograms showing the binned distributions of verbs according to their transitivity ratio in different languages}
  \label{fig:verb_dist_transitivity}
\end{sidewaysfigure}

Most distributions are bimodal with peak at both ends, which supports the overall cross-lingual validity of a binary conception of transitivity. But exceptions abound as well, among others Indonesian (unimodal with peak in the middle), Catalan, Galician, Spanish (unimodal with peak on right). And even between the bimodal distributions, they are notably rarely symmetric, with differing levels of skew towards either end. 

\subsection{Genetic and areal patterns in transitivity}

\input{tables/most_least_transitive_lang.tex}

Tab.~\ref{tab:most_tr_by_verb_percentage} and \ref{tab:most_tr_by_token_mean} list the most and least `transitive' languages in our study, respectively according to the transitive verb percentage and the token-level transitivity ratio. Recall that token-level transitivity ratios are favored over lexeme-level ones for cross-lingual comparison.

Among the most transitive languages are Romance (Catalan, Spanish, French, Galician) and Germanic (German, Norwegian, English, Danish) languages of Europe, Sinitic languages (Chinese, Classical Chinese, particularly when measured by token-level transitivity ratio), Indonesian, Hindi. On the opposite end of the spectrum are Hebrew, Irish, Japanese, as well as Baltic (Lithuanian, Latvian) and Slavic (Slovak, Russian, Polish) languages, which have the lowest transitivity ratios.

\begin{sidewaysfigure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/transitivity_europe.pdf}
  \caption{Mean transitivity ratios in languages of Europe}
  \label{fig:transitivity_europe}
\end{sidewaysfigure}

To look at any potential areal patterns in transitivity, the token-level transitivity ratio results for European languages are also mapped in Fig.~\ref{fig:transitivity_europe}. (A less Eurocentric study of areal patterns is unfortunately difficult for the lack of enough language samples in the UD.) A particularly high transitivity area can be observed in the Iberian peninsular as well as another relatively high transitivity area in the Balkans, in contrast to eastern and northern Europe with lower transitivity.

Where the languages overlap, these observations match well with those from \citet{say2014}'s survey of transitivity in European languages (as measured by the percentage of verbs that are transitive from a fixed list), who observed high transitivity areas in western Europe except Irish and south-western Balkans, and a corresponding low transitivity area in eastern Europe.

% Dutch appears to be a special case in several ways. \todo{case study: Dutch - divergence btwn two metrics, maybe due to ergative verbs? and diff btwn Dutch vs Afrikanns - significant grammatical differences?}

% diff between tr. verb -> token-level tr. ratio reflects also the fact that tr. verbs can often be used intransitively
% how "free" a language is to use tr. verb  intr./vice versa is another interesting further study 


\section{Experiment 2: Valency frame entropy}
\subsection{Valency frame and the efficient organization of the lexicon}

Cognitive linguists and typologists have increasingly sought to integrate the two approaches in the same functionalist research paradigm \citep{croft2016}. Among others is research that seeks to examine and explain language-internal and cross-linguistic features of human languages through the lens of communicative efficiency, at various levels including the lexicon, syntax and morphology (see \citealp{gibson2019} for a survey). 

An early example where efficiency is used to explain phenomena in human languages is the work of George Kingsley \citet{zipf1935,zipf1949}. He first studied what is now known as Zipf's law, namely the empirical observation that ``the magnitude of words tends, on the whole, [stands] in an inverse (not necessarily proportionate) relationship to the number of occurrences'', and sought to explain it through the principle of least effort. 

\citet{marslen-wilson1990,segui1982}
Psycholinguistic studies consistently show the effect of semantic and syntactic attributes of the verb on online sentence processing \citep{shapiro1987,collina2001}. 

However, results disagree whether subcategorization (syntactic) or thematic frames (semantic) has a greater effect on lexical processing. \citet{shapiro1987} reported that RTs for lexical decisions increased as the function of the number of thematic options instead of subcategorization options. In contrast, \citet{shetreet2007} reported on an fMRI study on Hebrew speakers, shows that the number of options in terms of subcategorization and thematic frames is better correlated to activity in the cortical areas that are associated with linguistic processing, as opposed to the number of complements or thematic frames.


\subsection{Measuring valency frame entropy in UD}

Besides the information about transitivity, UD annotations make available further information that are related to the morphosyntactic encoding of valency frame, including information about further dependents, word order, as well as case and adpositional markings. In this experiment, we make use of such information for a more fine-grained characterization of the variation in valency frame encoding for different verbs using entropy-based measures.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    \textbf{UD label} & \textbf{Dependent} \\
    \midrule
    \textsc{nsubj} & nominal subject \\
    \textsc{obj} & object \\
    \textsc{csubj} & clausal subject \\
    \textsc{ccomp} & clausal complement \\
    \textsc{xcomp}   & open clausal complement \\
    \midrule
    \textsc{iobj} & indirect object \\
    \textsc{obl} & oblique nominal \\
    \textsc{expl} & expletive \\
    \textsc{advmod} & adverbial modifier \\
    \textsc{advcl} & adverbial clause modifier \\
    \bottomrule
  \end{tabular}
  \caption{UD dependent labels included in valency frame extraction}\label{tab:ud-dependent-labels}
\end{table}

To compute the entropy measures, we need to first extract the \textbf{valency frame encoding} information from the UD annotations at a token level for each verb instance. We start by determining the scope of our investigation and select a list of dependency relations in Tab.~\ref{tab:ud-dependent-labels} which categorizes different syntactic dependencies that can be attached to the verb. These include core dependents, namely \textsc{nsubj} and \textsc{obj}, but also so-called non-core dependents as well. \todo{justification for that: syntactic classification of verb using non-core information, cite previous work} It is important to note here that it is the cross-lingual taxonomy of the dependencies that serves as the cross-linguistic basis of comparison and that the name of the labels, while linguistically informed, do not affect the entropy calculation in any way.

We extract information about (1) the presence or absence of dependency relations attached to a verb token, (2) the word order information, whether specific dependents precede or follow the head verb, and (3) morphological case marking on the dependents, if any.

\todo{example for what is to be extracted}

We propose \textbf{valency frame entropy}, conditioned on the verb, as a measure of the variation of valency frame encoding for a verb. This is formalized as the joint entropy of the relevant UD dependency relations, i.e., number of bits needed to encode the entire valency frame.

Let $X_1, X_2,\ldots,X_n$ be discrete random variables where each $X_k$ represents a UD dependency relation (e.g., \textsc{nsubj}, \textsc{obj}, \ldots). These variables have corresponding sample spaces (images? support?) $\mathcal{X}_1, \mathcal{X}_2, \ldots, \mathcal{X}_n$, which represent the possible levels or outcomes of each variable with regard to its presence, linearized order, and any case information.

Additionally, let there be a variable $V$ that represents the choice of a verb from the lexicon $\mathcal{V}$. We are first interested in calculating the entropy of each dependency relation given a specific verb $v \in \mathcal{V}$. This is defined as:

\begin{equation*}
  H(X_{k}|V=v)=
  -\sum\limits_{x_{k}\in{}\mathcal{X}_{k}}{P(x_{k}|V=v)\log_{2}{P(x_{k}|V=v)}}  
\end{equation*}

Here, $P(x_k | V = v)$ represents the conditional probability of the outcome $x_k$ of the dependency relation $X_k$ given that the verb variable $V$ takes on the value $v$. The entropy of $X_k$ is calculated by summing the products of these probabilities with their logarithms (base 2) taken, each corresponding to a possible outcome $x_k$.

The joint entropy of all dependent relations, denoted as $H_{\text{joint}}(X_1, X_2, \ldots, X_n | V = v)$, quantifies the uncertainty associated with the combined set of random variables $X_1, X_2, \ldots, X_n$, again given a specific value $v$ for the verb variable $V$. This is defined as:

\begin{equation*}
\begin{split}
 & H_{\text{joint}}(X_1, X_2, \ldots, X_n | V=v) \\
=& -\sum\limits_{x_1\in{}\mathcal{X}_1}\cdots\sum\limits_{x_n\in{}\mathcal{X}_n}{P(x_1, x_2, \ldots,x_{n}|V=v)\log_2P(x_1, x_2, \ldots,x_n|V=v)}
\end{split}
\end{equation*}

Here, $P(x_1, x_2, \ldots, x_n | V = v)$ represents the joint probability distribution of the outcomes $x_1, x_2, \ldots, x_n$ of the random variables $X_1, X_2, \ldots, X_n$, given that the verb variable $V$ takes on the value $v$. The joint entropy is calculated by summing the products of these joint probabilities with their logarithms (base 2) taken, each corresponding to a specific combination of outcomes $x_1, x_2, \ldots, x_n$.

% To reduce the effect of artifacts, we split each distribution randomly at a fixed ratio (e.g. $1:1$) into two, resulting in two distributions $X_1,\ldots,X_n$ and $X^{\prime}_{1},\ldots,X^{\prime}_{n}$. 

% For each of the argument slot, the cross-entropy between the distributions $X_k$ and $X_k^{\prime}$ with the same image $\mathcal{X}_{k}$ is

% $$
% H_{cross}{(X_k,X_k^{\prime}|V=v)}=
% -\sum\limits_{x\in{}\mathcal{X_{k}}}{P(x|V=v)\log_{2}{P^{\prime}(x|V=v)}}
% $$

% And the joint cross-entropy of  $X_1,\ldots,X_n$ and $X_1^{\prime},\ldots,X_n^{\prime}$ would be

% $$
% H_{joint-cross}(X_{1},\ldots,X_{n},X_{1}^{\prime},\ldots,X_{n}^{\prime}|V=v)=
% -\sum\limits_{x_1\in{}\mathcal{X}_1}\cdots\sum\limits_{x_n\in{}\mathcal{X}_n}{P(x_1,\ldots,x_{n}|V=v)\log_{2}P^{\prime}(x_1,\ldots,x_n|V=v)}
% $$


\subsection{Core and non-core dependents}



\begin{sidewaysfigure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/joint_entropy_freq.pdf}
  \caption{Scatter plots showing relationship between valency frame entropy and frequency rank of verbs, color shows number of frames associated with the verb}
  \label{fig:joint_entropy_freq}
\end{sidewaysfigure}

\section{Experiment 3: Word order and case information}
\subsection{Motivation}
\subsection{Experiment design: ablation studies}
\subsection{Results}

\section{Experiment 4: Verb entropy}
\subsection{Motivation}
\subsection{Experiment design}
\subsection{Results}

\section{Experiment 5: Verb-finalness}
\subsection{Motivation}
\subsection{Experiment design}
\subsection{Results}